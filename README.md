# mgpt2
Multilingual variant of the GPT-2 model. Tokenizer and model(with exact parameters from paper) fully implemented from scratch, pre-training and instruction tuning.

# References
- [Andrej Karpathy - Let's build the GPT Tokenizer](https://www.youtube.com/watch?v=zduSFxRajkE)
- [Andrej Karpathy - Let's reproduce GPT-2 (124M)](https://www.youtube.com/watch?v=l8pRSuU81PU)