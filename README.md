# mgpt2
Multilingual variant of the GPT-2 model. Tokenizer and model(with exact parameters from paper) implemented from scratch, pre-training and instruction tuning.

# References
- [Andrej Karpathy - Let's build the GPT Tokenizer](https://www.youtube.com/watch?v=zduSFxRajkE)
- [HF NLP Course - Tokenizers Library](https://huggingface.co/learn/nlp-course/en/chapter6/5)
- [Andrej Karpathy - Let's reproduce GPT-2 (124M)](https://www.youtube.com/watch?v=l8pRSuU81PU)
