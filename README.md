# mgpt2
Multilingual variant of the GPT-2 model. Tokenizer and model(with exact parameters from paper) fully implemented from scratch, pre-training and instruction tuning.
